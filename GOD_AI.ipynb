{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNrdJes54fVqEGnxDIePm0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sohampatel26/GOD-AI/blob/main/GOD_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwY222xg8Mdh"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from huggingface_hub import interpreter_login\n",
        "\n",
        "interpreter_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2Hr3oNR8ZGy",
        "outputId": "34d13cbc-8d06-4c15-bc12-61d7ff4c2eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your token (input will not be visible): ··········\n",
            "Add token as git credential? (Y/n) n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets"
      ],
      "metadata": {
        "id": "MwT8erSz9T3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset= load_dataset(\"nikita200/geeta\")"
      ],
      "metadata": {
        "id": "2g08_vQy-O0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_dataset)"
      ],
      "metadata": {
        "id": "-d9xZ6KqAAav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and test sets (e.g., 80% train, 20% test)\n",
        "train_test_split = final_dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
        "\n",
        "# Further split the test set into validation and test sets (e.g., 50% validation, 50% test)\n",
        "test_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Combine the splits into a single dataset dictionary\n",
        "final_dataset = {\n",
        "    'train': train_test_split['train'],\n",
        "    'validation': test_valid_split['train'],\n",
        "    'test': test_valid_split['test']\n",
        "}"
      ],
      "metadata": {
        "id": "e7ulSmfG5w-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "2nsn48OF546O",
        "outputId": "6eca4880-ea70-430d-fd27-3e44ca6e4f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f03fa410db04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset['train'][0]"
      ],
      "metadata": {
        "id": "KSEJ8mH_AREu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4030f9e-15be-4e9e-da77-1afce3d4c0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': '5. What is the significance of the battlefield in the Bhagavad Gita?',\n",
              " 'output': \"The battlefield in the Bhagavad Gita symbolizes the inner struggle of the human soul. The war between the Pandavas and Kauravas represents the eternal battle between good and evil, and the battlefield is the setting where the ultimate truth and wisdom are revealed to Arjuna by Krishna. It also represents the human mind and the constant struggle between our desires and our duty. The Gita teaches that true victory can only be achieved by conquering one's inner enemies and finding inner peace and harmony. The battlefield also serves as a reminder that life itself is a battle and one must face challenges with courage and determination.\",\n",
              " 'input': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "VKtSTWQZAGVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='microsoft/phi-2'\n",
        "device_map = {\"\": 0}\n",
        "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                      device_map=device_map,\n",
        "                                                      quantization_config=bnb_config,\n",
        "                                                      trust_remote_code=True,\n",
        "                                                      use_auth_token=True)"
      ],
      "metadata": {
        "id": "-rrmqCE-As8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "bc169b70-1560-49df-c91e-120876d44dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-beb0786d23a7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'microsoft/phi-2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m original_model = AutoModelForCausalLM.from_pretrained(model_name,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                                       \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                       \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3120\u001b[0m                 )\n\u001b[1;32m   3121\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3122\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   3123\u001b[0m                     \u001b[0;34m\"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3124\u001b[0m                 )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
      ],
      "metadata": {
        "id": "QuR-iTZRx5GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from transformers import set_seed, GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "def gen(model, prompt, max_length):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
        "    return [tokenizer.decode(outputs[0], skip_special_tokens=True)]\n",
        "\n",
        "# Select an index to test\n",
        "index = 10\n",
        "\n",
        "# Retrieve the instruction and output from the dataset\n",
        "prompt = final_dataset['test'][index]['instruction']\n",
        "summary = final_dataset['test'][index]['output']\n",
        "\n",
        "# Format the prompt for the model\n",
        "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
        "\n",
        "# Generate the output using the model\n",
        "res = gen(model, formatted_prompt, 100)\n",
        "\n",
        "# Process the output\n",
        "output = res[0].split('Output:\\n')[1] if 'Output:\\n' in res[0] else res[0]\n",
        "\n",
        "# Display the results\n",
        "dash_line = '-' * 100\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
      ],
      "metadata": {
        "id": "by17EuCmyAiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction','output')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruct: Give the best fit response for this question.\"\n",
        "    RESPONSE_KEY = \"### Output:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"\\n{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\"\n",
        "    input_context = f\"{sample['instruction']}\" if sample[\"instruction\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "R_WQJeW90wIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, final_dataset):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    final_dataset = final_dataset.map(create_prompt_formats)#, batched=True)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    final_dataset = final_dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=['instruction', 'output','input'],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    final_dataset = final_dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    final_dataset = final_dataset.shuffle(seed=seed)\n",
        "\n",
        "    return final_dataset"
      ],
      "metadata": {
        "id": "-xRqhmqD2cAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pre-process dataset\n",
        "max_length = get_max_length(original_model)\n",
        "print(max_length)\n",
        "\n",
        "train_dataset = preprocess_dataset(tokenizer, max_length,seed, final_dataset['train'])\n",
        "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, final_dataset['validation'])\n"
      ],
      "metadata": {
        "id": "QsdX_2J83g1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "yMJ2jznh7dgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "# Preparing the Model for QLoRA\n",
        "original_model = prepare_model_for_kbit_training(original_model)"
      ],
      "metadata": {
        "id": "oeDZgzOb3kMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=32, #Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        'q_proj',\n",
        "        'k_proj',\n",
        "        'v_proj',\n",
        "        'dense'\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "original_model.gradient_checkpointing_enable()\n",
        "\n",
        "peft_model = get_peft_model(original_model, config)"
      ],
      "metadata": {
        "id": "AksfdyH_7l1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "import transformers\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    warmup_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=25,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    do_eval=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    overwrite_output_dir = 'True',\n",
        "    group_by_length=True,\n",
        ")\n",
        "\n",
        "peft_model.config.use_cache = False\n",
        "\n",
        "peft_trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=peft_training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")"
      ],
      "metadata": {
        "id": "ETwFVAX_8faE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "SllaFfcc9WRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartConfig\n",
        "import torch\n",
        "\n",
        "# Define quantization configuration\n",
        "qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Define model configuration\n",
        "bnb_config = BartConfig(\n",
        "    quantization_enabled=True,\n",
        "    model_type='bart',\n",
        "    static=[True],\n",
        "    quant_types=['weight'],\n",
        "    int8=[False],\n",
        "    config_file=None\n",
        ")\n"
      ],
      "metadata": {
        "id": "I1hR6V9HiIqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "2OzVGImXn8FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "7BoaFiyItI2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "base_model_id = \"microsoft/phi-2\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
        "                                                      device_map='auto',\n",
        "                                                      quantization_config=bnb_config,\n",
        "                                                      trust_remote_code=True,\n",
        "                                                      use_auth_token=True)"
      ],
      "metadata": {
        "id": "_hw8tnpQmKd7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}